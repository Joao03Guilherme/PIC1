\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{nil}{}
\citation{Neifeld1993}
\citation{Foor1995}
\citation{Psaltis1984}
\citation{Dragulinescu2023}
\citation{VanderLugt1964}
\citation{Cruzeiro2024}
\citation{Sergioli2025}
\citation{Neifeld1993}
\citation{Foor1995}
\citation{Javidi1989}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {i}Machine learning and supervised learning}{1}{subsection.2.1}\protected@file@percent }
\newlabel{subsec:ml}{{i}{1}{Machine learning and supervised learning}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Input data.}{1}{paragraph*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training and test sets.}{1}{paragraph*.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces One representative example per class from the MNIST dataset. Each \(28\times 28\) grayscale image is flattened to a \(p=784\)-dimensional feature vector before the pre-processing pipeline (centering, standardisation, PCA).\relax }}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mnist-examples}{{1}{1}{One representative example per class from the MNIST dataset. Each \(28\times 28\) grayscale image is flattened to a \(p=784\)-dimensional feature vector before the pre-processing pipeline (centering, standardisation, PCA).\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Pre-processing.}{1}{paragraph*.4}\protected@file@percent }
\citation{Sergioli2025}
\@writefile{toc}{\contentsline {paragraph}{Balanced accuracy.}{2}{paragraph*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classical Nearest-Mean Classifier (CNM-C).}{2}{paragraph*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Radial-Basis-Function Classifier (RBF-C).}{2}{paragraph*.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Ilustration of the CNM-C decision rule. The training set is split into two classes and the centroids are computed. The nearest centroid to the test sample is selected as the predicted class.\relax }}{2}{figure.caption.7}\protected@file@percent }
\newlabel{fig:nearest_mean_classifier}{{2}{2}{Ilustration of the CNM-C decision rule. The training set is split into two classes and the centroids are computed. The nearest centroid to the test sample is selected as the predicted class.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {ii}Quantum-inspired classification}{2}{subsection.2.2}\protected@file@percent }
\newlabel{subsec:qic}{{ii}{2}{Quantum-inspired classification}{subsection.2.2}{}}
\citation{Sergioli2025}
\citation{Javidi1989}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Architecture of the RBF network. The input layer feeds into a hidden layer of radial units, each centered at \(\mathbf  {t}_i\) with width \(\sigma _i\). The output layer combines the gaussian responses through learned weights \(w_{ij}\), followed by a winner-takes-all decision stage to determine the predicted class.\relax }}{3}{figure.caption.9}\protected@file@percent }
\newlabel{fig:rbf-net}{{3}{3}{Architecture of the RBF network. The input layer feeds into a hidden layer of radial units, each centered at \(\mathbf {t}_i\) with width \(\sigma _i\). The output layer combines the gaussian responses through learned weights \(w_{ij}\), followed by a winner-takes-all decision stage to determine the predicted class.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Density--pattern encodings.}{3}{paragraph*.10}\protected@file@percent }
\newlabel{par:encodings}{{ii}{3}{Density--pattern encodings}{paragraph*.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantum centroid.}{3}{paragraph*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantum distance measure.}{3}{paragraph*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantum-inspired nearest-mean classifier (QNM-C).}{3}{paragraph*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {iii}Joint Transform Correlator}{4}{subsection.2.3}\protected@file@percent }
\newlabel{subsec:jtc}{{iii}{4}{Joint Transform Correlator}{subsection.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Optical setup for the joint transform correlator. The SLM displays the reference and query patterns side by side.\relax }}{4}{figure.caption.14}\protected@file@percent }
\newlabel{fig:optical-classifier}{{4}{4}{Optical setup for the joint transform correlator. The SLM displays the reference and query patterns side by side.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Operational principle.}{4}{paragraph*.15}\protected@file@percent }
\newlabel{fig:corr-shifted}{{5a}{4}{Original image vs.\ shifted copy\relax }{figure.caption.16}{}}
\newlabel{sub@fig:corr-shifted}{{a}{4}{Original image vs.\ shifted copy\relax }{figure.caption.16}{}}
\newlabel{fig:corr-random}{{5b}{4}{Original image vs.\ random image\relax }{figure.caption.16}{}}
\newlabel{sub@fig:corr-random}{{b}{4}{Original image vs.\ random image\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Correlation-plane intensity produced by the joint transform correlator: (a) shows two strong off-axis peaks because the query is a shifted version of the reference, whereas (b) shows no significant peaks for an unrelated image.\relax }}{4}{figure.caption.16}\protected@file@percent }
\newlabel{fig:jtc-diagram}{{5}{4}{Correlation-plane intensity produced by the joint transform correlator: (a) shows two strong off-axis peaks because the query is a shifted version of the reference, whereas (b) shows no significant peaks for an unrelated image.\relax }{figure.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Classification accuracy (mean $\pm $ std.)\relax }}{4}{table.caption.17}\protected@file@percent }
\newlabel{tab:cls-results}{{1}{4}{Classification accuracy (mean $\pm $ std.)\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {iv}Optical implementations of the classifiers}{5}{subsection.2.4}\protected@file@percent }
\newlabel{subsec:optical-qnm}{{iv}{5}{Optical implementations of the classifiers}{subsection.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Results}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Data Encoding}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {i}Gram Matrix Encoding}{5}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Gram matrix encoding $\rho $ of the digit 1 image\relax }}{5}{figure.caption.18}\protected@file@percent }
\newlabel{fig:density_matrix}{{6}{5}{Gram matrix encoding $\rho $ of the digit 1 image\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Radial Basis Function Network}{5}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {i}Exact Interpolation}{5}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {ii}Classification}{5}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {iii}Training with Least Squares}{5}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Choosing RBF Centers}{5}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Example of a Gaussian RBF centered at $\mu $\relax }}{5}{figure.caption.19}\protected@file@percent }
\newlabel{fig:gaussian_rbf}{{7}{5}{Example of a Gaussian RBF centered at $\mu $\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Implementation Notes}{5}{section.7}\protected@file@percent }
\bibdata{references}
\bibcite{Cruzeiro2024}{1}
\bibcite{Dragulinescu2023}{2}
\bibcite{Foor1995}{3}
\bibcite{Javidi1989}{4}
\bibcite{Neifeld1993}{5}
\bibcite{Psaltis1984}{6}
\bibcite{Sergioli2025}{7}
\bibcite{VanderLugt1964}{8}
\bibstyle{plain}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Optical Im}{6}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IX}Conclusion}{6}{section.9}\protected@file@percent }
\gdef \@abspage@last{7}
